{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Index, RangeIndex\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from causalAssembly.models_dag import ProductionLineGraph\n",
    "from causalAssembly.drf_fitting import fit_drf\n",
    "\n",
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.datasets import IIDSimulation, DAG\n",
    "from castle.algorithms import Notears\n",
    "# adjust GAE algorithm\n",
    "# from castle.algorithms import GAE\n",
    "from castle.algorithms import DirectLiNGAM\n",
    "from castle.algorithms import PC\n",
    "\n",
    "from dagma.linear import DagmaLinear\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FREQUENCY = 100\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Referred from:\n",
    "    - https://stackoverflow.com/questions/38469632/tensorflow-non-repeatable-results\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    try:\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def compute_h(w_adj):\n",
    "\n",
    "    d = w_adj.shape[0]\n",
    "    h = torch.trace(torch.matrix_exp(w_adj * w_adj)) - d\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "class Tensor(np.ndarray):\n",
    "    \"\"\"A subclass of numpy.ndarray.\n",
    "\n",
    "    This subclass has all attributes and methods of numpy.ndarray\n",
    "    with two additional, user-defined attributes: `index` and `columns`.\n",
    "\n",
    "    It can be used in the same way as a standard numpy.ndarray.\n",
    "    However, after performing any operations on the Tensor (e.g., slicing,\n",
    "    transposing, arithmetic, etc.), the user-defined attribute values of\n",
    "    `index` and `columns` will be lost and replaced with a numeric indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    object: array-like\n",
    "        Multiple list, ndarray, DataFrame\n",
    "    index : Index or array-like\n",
    "        Index to use for resulting tensor. Will default to RangeIndex if\n",
    "        no indexing information part of input data and no index provided.\n",
    "    columns : Index or array-like\n",
    "        Column labels to use for resulting tensor. Will default to\n",
    "        RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Create a Tensor from a list or numpy.ndarray.\n",
    "\n",
    "    >>> x = [[0, 3, 8, 1],\n",
    "    ...      [8, 4, 1, 9],\n",
    "    ...      [7, 3, 3, 7]]\n",
    "\n",
    "    Or\n",
    "\n",
    "    >>> x = np.random.randint(0, 10, size=12).reshape((3, 4))\n",
    "    >>> arr = Tensor(x)\n",
    "    >>> arr\n",
    "    Tensor([[0, 3, 8, 1],\n",
    "            [8, 4, 1, 9],\n",
    "            [7, 3, 3, 7]])\n",
    "    >>> arr.index\n",
    "    RangeIndex(start=0, stop=3, step=1)\n",
    "    >>> list(arr.index)\n",
    "    [0, 1, 2]\n",
    "    >>> arr.columns\n",
    "    RangeIndex(start=0, stop=4, step=1)\n",
    "    >>> list(arr.columns)\n",
    "    [0, 1, 2, 3]\n",
    "\n",
    "    `index` and `columns` can be set using kwargs.\n",
    "\n",
    "    >>> arr = Tensor(x, index=list('XYZ'), columns=list('ABCD'))\n",
    "    >>> arr\n",
    "    Tensor([[6, 1, 8, 9],\n",
    "            [1, 5, 2, 1],\n",
    "            [5, 9, 4, 5]])\n",
    "    >>> arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> arr.columns\n",
    "    Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "\n",
    "    Or a value can be assigned to `arr.index` or `arr.columns`,\n",
    "    but it must be an `Iterable`.\n",
    "\n",
    "    >>> arr.index = list('xyz')\n",
    "    >>> arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> arr.columns = list('abcd')\n",
    "    >>> arr.columns\n",
    "    Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "\n",
    "    A Tensor can also be created from a pandas.DataFrame.\n",
    "\n",
    "    >>> x = pd.DataFrame(np.random.randint(0, 10, size=12).reshape((3, 4)),\n",
    "    ...                  index=list('xyz'),\n",
    "    ...                  columns=list('abcd'))\n",
    "    >>> x\n",
    "       a  b  c  d\n",
    "    x  6  1  8  9\n",
    "    y  1  5  2  1\n",
    "    z  5  9  4  5\n",
    "    >>> arr = Tensor(x)\n",
    "    >>> arr\n",
    "    Tensor([[6, 1, 8, 9],\n",
    "            [1, 5, 2, 1],\n",
    "            [5, 9, 4, 5]])\n",
    "    >>> arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> arr.columns\n",
    "    Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "\n",
    "    It's possible to use any method of numpy.ndarray on the Tensor,\n",
    "    such as `sum`, `@`, etc.\n",
    "\n",
    "    >>> arr.sum(axis=0)\n",
    "    Tensor([15, 10, 12, 17])\n",
    "    >>> arr @ arr.T\n",
    "    Tensor([[ 74,  29,  40],\n",
    "            [ 29, 162, 134],\n",
    "            [ 40, 134, 116]])\n",
    "\n",
    "    If the Tensor is sliced, the values of `index` and `columns` will disappear,\n",
    "    and new values of type `RangeIndex` will be created.\n",
    "\n",
    "    >>> new_arr = arr[:, 1:3]\n",
    "    >>> new_arr\n",
    "    Tensor([[1, 8],\n",
    "            [5, 2],\n",
    "            [9, 4]])\n",
    "    >>> new_arr.index\n",
    "    RangeIndex(start=0, stop=3, step=1)\n",
    "    >>> new_arr.columns\n",
    "    RangeIndex(start=0, stop=2, step=1)\n",
    "\n",
    "    If you want to retain the values of `index` and `columns`,\n",
    "    you can reassign them.\n",
    "\n",
    "    >>> new_arr.index = arr.index[:]\n",
    "    >>> new_arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "\n",
    "    >>> new_arr.columns = arr.columns[1:3]\n",
    "    >>> new_arr.columns\n",
    "    Index(['b', 'c'], dtype='object')\n",
    "\n",
    "    We recommend performing slicing operations in the following way\n",
    "    to keep the `index` and `columns` values.\n",
    "\n",
    "    >>> new_arr = Tensor(array=arr[:, 1:3],\n",
    "    ...                  index=arr.index[:, 1:3],\n",
    "    ...                  columns=arr.columns[:, 1:3])\n",
    "    >>> new_arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> new_arr.columns\n",
    "    Index(['b', 'c'], dtype='object')\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, object=None, index=None, columns=None):\n",
    "\n",
    "        if object is None:\n",
    "            raise TypeError(\"Tensor() missing required argument 'object' (pos 0)\")\n",
    "        elif isinstance(object, list):\n",
    "            object = np.array(object)\n",
    "        elif isinstance(object, pd.DataFrame):\n",
    "            index = object.index\n",
    "            columns = object.columns\n",
    "            object = object.values\n",
    "        elif isinstance(object, (np.ndarray, cls)):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Type of the required argument 'object' must be array-like.\"\n",
    "            )\n",
    "        if index is None:\n",
    "            index = range(object.shape[0])\n",
    "        if columns is None:\n",
    "            columns = range(object.shape[1])\n",
    "        obj = np.asarray(object).view(cls)\n",
    "        obj.index = index\n",
    "        obj.columns = columns\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj):\n",
    "        if obj is None: return\n",
    "        if self.ndim == 0: return\n",
    "        elif self.ndim == 1:\n",
    "            self.columns = RangeIndex(0, 1, step=1, dtype=int)\n",
    "        else:\n",
    "            self.columns = RangeIndex(0, self.shape[1], step=1, dtype=int)\n",
    "        self.index = RangeIndex(0, self.shape[0], step=1, dtype=int)\n",
    "\n",
    "    @property\n",
    "    def index(self):\n",
    "        return self._index\n",
    "\n",
    "    @index.setter\n",
    "    def index(self, value):\n",
    "        assert isinstance(value, Iterable)\n",
    "        if len(list(value)) != self.shape[0]:\n",
    "            raise ValueError(\"Size of value is not equal to the shape[0].\")\n",
    "        self._index = Index(value)\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "\n",
    "    @columns.setter\n",
    "    def columns(self, value):\n",
    "        assert isinstance(value, Iterable)\n",
    "        if (self.ndim > 1 and len(list(value)) != self.shape[1]):\n",
    "            raise ValueError(\"Size of value is not equal to the shape[1].\")\n",
    "        self._columns = Index(value)\n",
    "\n",
    "\n",
    "class ALTrainer(object):\n",
    "\n",
    "    def __init__(self, n, d, model, lr, init_iter, alpha, beta, rho, rho_thresh,\n",
    "                 h_thresh, l1_penalty, gamma, early_stopping,\n",
    "                 early_stopping_thresh, seed, device=None):\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.init_iter = init_iter\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta  # rho_multiply\n",
    "        self.rho = rho\n",
    "        self.rho_thresh = rho_thresh\n",
    "        self.h_thresh = h_thresh  # 1e-8\n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.gamma = gamma\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_thresh = early_stopping_thresh\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                          lr=self.lr)\n",
    "\n",
    "    def train(self, x, epochs, update_freq):\n",
    "\n",
    "        alpha, beta, rho = self.alpha, self.beta, self.rho\n",
    "        h, h_new = np.inf, np.inf\n",
    "        prev_w_est, prev_mse = None, np.inf\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logging.info(f'Current epoch: {epoch}==================')\n",
    "            while rho < self.rho_thresh:\n",
    "                mse_new, h_new, w_new = self.train_step(x,\n",
    "                                                        update_freq,\n",
    "                                                        alpha,\n",
    "                                                        rho)\n",
    "                if h_new > self.gamma * h:\n",
    "                    rho *= self.beta\n",
    "                else:\n",
    "                    break\n",
    "            logging.info(f'Current        h: {h_new}')\n",
    "\n",
    "            if self.early_stopping:\n",
    "                if (mse_new / prev_mse > self.early_stopping_thresh\n",
    "                        and h_new <= 1e-7):\n",
    "                    return prev_w_est\n",
    "                else:\n",
    "                    prev_w_est = w_new\n",
    "                    prev_mse = mse_new\n",
    "\n",
    "            # update rules\n",
    "            w_est, h = w_new, h_new\n",
    "            alpha += rho * h_new.detach().cpu()\n",
    "\n",
    "            if h <= self.h_thresh and epoch > self.init_iter:\n",
    "                break\n",
    "\n",
    "        return w_est\n",
    "\n",
    "\n",
    "    def train_step(self, x, update_freq, alpha, rho):\n",
    "\n",
    "        curr_mse, curr_h, w_adj = None, None, None\n",
    "        for _ in range(update_freq):\n",
    "            torch.manual_seed(self.seed)\n",
    "            curr_mse, w_adj = self.model(x)\n",
    "            curr_h = compute_h(w_adj)\n",
    "            loss = ((0.5 / self.n) * curr_mse\n",
    "                    + self.l1_penalty * torch.norm(w_adj, p=1)\n",
    "                    + alpha * curr_h + 0.5 * rho * curr_h * curr_h)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if _ % LOG_FREQUENCY == 0:\n",
    "                logging.info(f'Current loss in step {_}: {loss.detach()}')\n",
    "\n",
    "        return curr_mse, curr_h, w_adj\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward neural networks----MLP\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, layers, units, output_dim,\n",
    "                 activation=None, device=None) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        # self.desc = desc\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = layers\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.device = device\n",
    "\n",
    "        mlp = []\n",
    "        for i in range(layers):\n",
    "            input_size = units\n",
    "            if i == 0:\n",
    "                input_size = input_dim\n",
    "            weight = nn.Linear(in_features=input_size,\n",
    "                               out_features=self.units,\n",
    "                               bias=True,\n",
    "                               device=self.device)\n",
    "            mlp.append(weight)\n",
    "            if activation is not None:\n",
    "                mlp.append(activation)\n",
    "        out_layer = nn.Linear(in_features=self.units,\n",
    "                              out_features=self.output_dim,\n",
    "                              bias=True,\n",
    "                              device=self.device)\n",
    "        mlp.append(out_layer)\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "\n",
    "        x_ = x.reshape(-1, self.input_dim)\n",
    "        output = self.mlp(x_)\n",
    "\n",
    "        return output.reshape(x.shape[0], -1, self.output_dim)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d, input_dim, hidden_layers=3, hidden_dim=16,\n",
    "                 activation=nn.ReLU(), device=None):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.d = d\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = MLP(input_dim=self.input_dim,\n",
    "                           layers=self.hidden_layers,\n",
    "                           units=self.hidden_dim,\n",
    "                           output_dim=self.hidden_dim,\n",
    "                           activation=self.activation,\n",
    "                           device=self.device)\n",
    "        self.decoder = MLP(input_dim=self.hidden_dim,\n",
    "                           layers=self.hidden_layers,\n",
    "                           units=self.hidden_dim,\n",
    "                           output_dim=self.input_dim,\n",
    "                           activation=self.activation,\n",
    "                           device=self.device)\n",
    "\n",
    "        w = torch.nn.init.uniform_(torch.empty(self.d, self.d,),\n",
    "                                   a=-0.1, b=0.1)\n",
    "        self.w = torch.nn.Parameter(w.to(device=self.device))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.w_adj = self._preprocess_graph(self.w)\n",
    "\n",
    "        out = self.encoder(x)\n",
    "        out = torch.einsum('ijk,jl->ilk', out, self.w_adj)\n",
    "        # x_est = self.decoder(out)\n",
    "        x_est = torch.sigmoid(self.decoder(out))\n",
    "\n",
    "        # mse_loss = torch.square(torch.norm(x - x_est, p=2))\n",
    "        mse_loss = F.binary_cross_entropy(x, x_est)\n",
    "\n",
    "\n",
    "        return mse_loss, self.w_adj\n",
    "\n",
    "    def _preprocess_graph(self, w_adj):\n",
    "\n",
    "        return (1. - torch.eye(w_adj.shape[0], device=self.device)) * w_adj\n",
    "\n",
    "\n",
    "class GAE:\n",
    "    \"\"\"\n",
    "    GAE Algorithm.\n",
    "    A gradient-based algorithm using graph autoencoder to model non-linear\n",
    "    causal relationships.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim: int, default: 1\n",
    "        dimension of vector for x\n",
    "    hidden_layers: int, default: 1\n",
    "        number of hidden layers for encoder and decoder\n",
    "    hidden_dim: int, default: 4\n",
    "        hidden size for mlp layer\n",
    "    activation: callable, default: nn.LeakyReLU(0.05)\n",
    "        nonlinear functional\n",
    "    epochs: int, default: 10\n",
    "        Number of iterations for optimization problem\n",
    "    update_freq: int, default: 3000\n",
    "        Number of steps for each iteration\n",
    "    init_iter: int, default: 3\n",
    "        Initial iteration to disallow early stopping\n",
    "    lr: float, default: 1e-3\n",
    "        learning rate\n",
    "    alpha: float, default: 0.0\n",
    "        Lagrange multiplier\n",
    "    beta: float, default: 2.0\n",
    "        Multiplication to amplify rho each time\n",
    "    init_rho: float, default: 1.0\n",
    "        Initial value for rho\n",
    "    rho_thresh: float, default: 1e30\n",
    "        Threshold for rho\n",
    "    gamma: float, default: 0.25\n",
    "        Threshold for h\n",
    "    penalty_lambda: float, default: 0.0\n",
    "        L1 penalty for sparse graph. Set to 0.0 to disable\n",
    "    h_thresh: float, default: 1e-8\n",
    "        Tolerance of optimization problem\n",
    "    graph_thresh: float, default: 0.3\n",
    "        Threshold to filter out small values in graph\n",
    "    early_stopping: bool, default: False\n",
    "        Whether to use early stopping\n",
    "    early_stopping_thresh: float, default: 1.0\n",
    "        Threshold ratio for early stopping\n",
    "    seed: int, default: 1230\n",
    "        Reproducibility, must be int\n",
    "    device_type: str, default: 'cpu'\n",
    "        'cpu' or 'gpu'\n",
    "    device_ids: int or str, default '0'\n",
    "        CUDA devices, it's effective when ``use_gpu`` is True.\n",
    "        For single-device modules, ``device_ids`` can be int or str,\n",
    "        e.g. 0 or '0', For multi-device modules, ``device_ids`` must be str,\n",
    "        format like '0, 1'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim=1,\n",
    "                 hidden_layers=1,\n",
    "                 hidden_dim=4,\n",
    "                 activation=torch.nn.LeakyReLU(0.05),\n",
    "                 epochs=10,\n",
    "                 update_freq=3000,\n",
    "                 init_iter=3,\n",
    "                 lr=1e-3,\n",
    "                 alpha=0.0,\n",
    "                 beta=2.0,\n",
    "                 init_rho=1.0,\n",
    "                 rho_thresh=1e30,\n",
    "                 gamma=0.25,\n",
    "                 penalty_lambda=0.0,\n",
    "                 h_thresh=1e-8,\n",
    "                 graph_thresh=0.3,\n",
    "                 early_stopping=False,\n",
    "                 early_stopping_thresh=1.0,\n",
    "                 seed=1230,\n",
    "                 device_type='cpu',\n",
    "                 device_ids='0'):\n",
    "\n",
    "        super(GAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.epochs = epochs\n",
    "        self.update_freq = update_freq\n",
    "        self.init_iter = init_iter\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.init_rho = init_rho\n",
    "        self.rho_thresh = rho_thresh\n",
    "        self.gamma = gamma\n",
    "        self.penalty_lambda = penalty_lambda\n",
    "        self.h_thresh = h_thresh\n",
    "        self.graph_thresh = graph_thresh\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_thresh = early_stopping_thresh\n",
    "        self.seed = seed\n",
    "        self.device_type = device_type\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            logging.info('GPU is available.')\n",
    "        else:\n",
    "            logging.info('GPU is unavailable.')\n",
    "            if self.device_type == 'gpu':\n",
    "                raise ValueError(\"GPU is unavailable, \"\n",
    "                                 \"please set device_type = 'cpu'.\")\n",
    "        if self.device_type == 'gpu':\n",
    "            if self.device_ids:\n",
    "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
    "            device = torch.device('cuda')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.device = device\n",
    "\n",
    "    def learn(self, data, columns=None, **kwargs):\n",
    "\n",
    "        x = torch.from_numpy(data)\n",
    "\n",
    "        self.n, self.d = x.shape[:2]\n",
    "        if x.ndim == 2:\n",
    "            x = x.reshape((self.n, self.d, 1))\n",
    "            self.input_dim = 1\n",
    "        elif x.ndim == 3:\n",
    "            self.input_dim = x.shape[2]\n",
    "\n",
    "        w_est = self._gae(x).detach().cpu().numpy()\n",
    "\n",
    "        self.weight_causal_matrix = Tensor(w_est,\n",
    "                                           index=columns,\n",
    "                                           columns=columns)\n",
    "        causal_matrix = (abs(w_est) > self.graph_thresh).astype(int)\n",
    "        self.causal_matrix = Tensor(causal_matrix,\n",
    "                                    index=columns,\n",
    "                                    columns=columns)\n",
    "\n",
    "    def _gae(self, x):\n",
    "\n",
    "        set_seed(self.seed)\n",
    "        model = AutoEncoder(d=self.d,\n",
    "                            input_dim=self.input_dim,\n",
    "                            hidden_layers=self.hidden_layers,\n",
    "                            hidden_dim=self.hidden_dim,\n",
    "                            activation=self.activation,\n",
    "                            device=self.device,\n",
    "                            )\n",
    "        trainer = ALTrainer(n=self.n,\n",
    "                            d=self.d,\n",
    "                            model=model,\n",
    "                            lr=self.lr,\n",
    "                            init_iter=self.init_iter,\n",
    "                            alpha=self.alpha,\n",
    "                            beta=self.beta,\n",
    "                            rho=self.init_rho,\n",
    "                            l1_penalty=self.penalty_lambda,\n",
    "                            rho_thresh=self.rho_thresh,\n",
    "                            h_thresh=self.h_thresh,  # 1e-8\n",
    "                            early_stopping=self.early_stopping,\n",
    "                            early_stopping_thresh=self.early_stopping_thresh,\n",
    "                            gamma=self.gamma,\n",
    "                            seed=self.seed,\n",
    "                            device=self.device)\n",
    "        w_est = trainer.train(x=x,\n",
    "                              epochs=self.epochs,\n",
    "                              update_freq=self.update_freq)\n",
    "        w_est = w_est / torch.max(abs(w_est))\n",
    "\n",
    "        return w_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'testing_results.csv' already exists.\n"
     ]
    }
   ],
   "source": [
    "#  set up testing collection df\n",
    "\n",
    "testing_results_columns = ['date', 'dataset', 'subset', 'n', 'method', 'runtime', 'hyperparams',\n",
    "                                        'fdr', 'tpr', 'fpr', 'shd', 'nnz', 'precision', 'recall', 'F1', 'gscore']\n",
    "\n",
    "# Check if the file exists\n",
    "try:\n",
    "    with open('testing_results.csv', 'r') as csvfile:\n",
    "        print(\"File 'testing_results.csv' already exists.\")\n",
    "except FileNotFoundError:\n",
    "    # Create a new file with the specified columns\n",
    "    with open('testing_results.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(testing_results_columns)\n",
    "        print(\"New file 'testing_results.csv' created with the specified columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to save the date to the testing_results.csv\n",
    "\n",
    "def add_results(dataset, subset, n, method, runtime, hyperparams, metrics):\n",
    "    with open('testing_results.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([time.strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "                        dataset, \n",
    "                        subset,\n",
    "                        n, \n",
    "                        method, \n",
    "                        runtime, \n",
    "                        hyperparams,\n",
    "                        metrics['fdr'],\n",
    "                        metrics['tpr'],\n",
    "                        metrics['fpr'],\n",
    "                        metrics['shd'],\n",
    "                        metrics['nnz'],\n",
    "                        metrics['precision'],\n",
    "                        metrics['recall'],\n",
    "                        metrics['F1'],\n",
    "                        metrics['gscore']]\n",
    "                        )\n",
    "        print(\"Results added to 'testing_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and save a graph plot next to a heatmap\n",
    "\n",
    "def plot_graph_adjacency_matrix(ground_truth, save_name):\n",
    "\n",
    "    # create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(15, 8), ncols=2)\n",
    "\n",
    "    # subplot for graph\n",
    "    ax1.set_title('Graph', y= 1.05)\n",
    "    G = nx.from_numpy_matrix(ground_truth, create_using=nx.DiGraph)\n",
    "    nx.draw(G, ax=ax1, with_labels=True)\n",
    "\n",
    "    # subplot for heatmap\n",
    "    ax2.set_title('Adjazenzmatrix Heat Map', y= 1.05)\n",
    "    map1 = ax2.imshow(ground_truth, cmap='Greys', interpolation='none')\n",
    "    cbar = fig.colorbar(map1, ax=ax2)\n",
    "    cbar.ax.set_position([cbar.ax.get_position().x0, ax2.get_position().y0,\n",
    "                        cbar.ax.get_position().width, ax2.get_position().height])\n",
    "\n",
    "    # Ensure subplots have same height and position in y direction\n",
    "    ax1_height = ax2.get_position().height\n",
    "    ax1.set_position([ax1.get_position().x0, ax2.get_position().y0,\n",
    "                    ax1.get_position().width, ax1_height])\n",
    "    ax2.set_position([ax2.get_position().x0, ax2.get_position().y0,\n",
    "                    ax2.get_position().width, ax1_height])\n",
    "\n",
    "    # save\n",
    "    plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to only save a heat map\n",
    "\n",
    "def plot_heatmap(ground_truth, method, save_name):\n",
    "    \n",
    "    # create figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # subplot for heatmap\n",
    "    ax.set_title(method, y= 1.05)\n",
    "    map1 = ax.imshow(ground_truth, cmap='Greys', interpolation='none')\n",
    "    cbar = fig.colorbar(map1, ax=ax)\n",
    "    cbar.ax.set_position([cbar.ax.get_position().x0, ax.get_position().y0,\n",
    "                        cbar.ax.get_position().width, ax.get_position().height])\n",
    "\n",
    "    # save\n",
    "    plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test setup\n",
    "\n",
    "def run_tests(dataset, subset, algorithms, sample_sizes):\n",
    "    \n",
    "    ground_truth = pd.read_csv(f'data/{dataset}/{subset}/{subset}_ground_truth.csv').to_numpy()\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "\n",
    "        data = pd.read_csv(f'data/{dataset}/{subset}/{subset}_data_{sample_size}.csv').to_numpy()\n",
    "\n",
    "        # go over all algorithms\n",
    "        for algorithm in algorithms:\n",
    "\n",
    "            # set different params for the different models\n",
    "            match algorithm:\n",
    "\n",
    "                case 'NotearsLinear':\n",
    "                    model = Notears(loss_type='logistic')\n",
    "                    start_time = time.time()\n",
    "                    model.learn(data)\n",
    "                    end_time = time.time()             \n",
    "\n",
    "                case 'DagmaLinear':\n",
    "                    model = DagmaLinear(loss_type='logistic')\n",
    "                    start_time = time.time()\n",
    "                    W_est = (model.fit(data, max_iter=1e6) > 0.5).astype(\"int32\")\n",
    "                    end_time = time.time()\n",
    "\n",
    "                case 'GAE':\n",
    "                    model = GAE(epochs=3)\n",
    "                    start_time = time.time()\n",
    "                    model.learn(data)\n",
    "                    end_time = time.time()\n",
    "\n",
    "                case 'DirectLiNGAM':\n",
    "                    model = DirectLiNGAM()\n",
    "                    start_time = time.time()\n",
    "                    model.learn(data)\n",
    "                    end_time = time.time()\n",
    "\n",
    "                case 'PC':\n",
    "                    model = PC(variant = 'stable')\n",
    "                    start_time = time.time()\n",
    "                    model.learn(data)\n",
    "                    end_time = time.time()\n",
    "\n",
    "\n",
    "            # calculate runtime\n",
    "            runtime = end_time - start_time\n",
    "\n",
    "            # plot comparison graph\n",
    "            name = f'{algorithm}_{sample_size}'\n",
    "            print(name)\n",
    "            save_name1 = f'plots/adj_matrix_comparison/{dataset}/{subset}/{name}.png'\n",
    "\n",
    "            # plot only heatmap for later comparison\n",
    "            save_name2 = f'plots/adj_matrix/{dataset}/{subset}/{name}.png'\n",
    "\n",
    "            # save adj_matrix as csv\n",
    "            save_name3 = f'plots/adj_matrix_csv/{dataset}/{subset}/{name}.csv'\n",
    "\n",
    "            match algorithm:\n",
    "                case 'NotearsLinear':\n",
    "                    met = MetricsDAG(model.causal_matrix, ground_truth)\n",
    "                    GraphDAG(model.causal_matrix, ground_truth, save_name = save_name1)\n",
    "                    plot_heatmap(model.causal_matrix, method = algorithm, save_name = save_name2)\n",
    "                    df = pd.DataFrame(np.array(model.causal_matrix.tolist()).reshape(model.causal_matrix.shape))\n",
    "                case 'DagmaLinear':\n",
    "                    met = MetricsDAG(W_est, ground_truth)\n",
    "                    GraphDAG(W_est, ground_truth, save_name = save_name1)\n",
    "                    plot_heatmap(W_est, method = algorithm, save_name = save_name2)\n",
    "                    df = pd.DataFrame(np.array(W_est.tolist()).reshape(W_est.shape))\n",
    "                case 'GAE':\n",
    "                    met = MetricsDAG(model.causal_matrix, ground_truth)\n",
    "                    GraphDAG(model.causal_matrix, ground_truth, save_name = save_name1)\n",
    "                    plot_heatmap(model.causal_matrix, method = algorithm, save_name = save_name2)\n",
    "                    df = pd.DataFrame(np.array(model.causal_matrix.tolist()).reshape(model.causal_matrix.shape))\n",
    "                case 'DirectLiNGAM':\n",
    "                    met = MetricsDAG(model.causal_matrix, ground_truth)\n",
    "                    GraphDAG(model.causal_matrix, ground_truth, save_name = save_name1)\n",
    "                    plot_heatmap(model.causal_matrix, method = algorithm, save_name = save_name2)\n",
    "                    df = pd.DataFrame(np.array(model.causal_matrix.tolist()).reshape(model.causal_matrix.shape))\n",
    "                case 'PC':\n",
    "                    met = MetricsDAG(model.causal_matrix, ground_truth)\n",
    "                    GraphDAG(model.causal_matrix, ground_truth, save_name = save_name1)\n",
    "                    plot_heatmap(model.causal_matrix, method = algorithm, save_name = save_name2)\n",
    "                    df = pd.DataFrame(np.array(model.causal_matrix.tolist()).reshape(model.causal_matrix.shape))\n",
    "\n",
    "            # save adj matrix to csv\n",
    "            df.to_csv(save_name3, index=False)\n",
    "\n",
    "            # save metrics to csv\n",
    "            add_results(dataset, subset, sample_size, algorithm, runtime, None, met.metrics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define testing parameters\n",
    "\n",
    "algorithms = ['NotearsLinear'\n",
    "              , 'DagmaLinear'\n",
    "              , 'GAE'\n",
    "            #   , 'DirectLiNGAM'\n",
    "            #   , 'PC'\n",
    "              ]\n",
    "\n",
    "sample_sizes = [500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "datasets = ['causalAssembly', 'IID']\n",
    "subsets = ['custom_line3', 'IID3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'custom_line3_data_500.csv' exists.\n",
      "A file for the combination of causalAssembly and IID3 does not exist.\n",
      "A file for the combination of IID and custom_line3 does not exist.\n",
      "File 'IID3_data_500.csv' exists.\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    for subset in subsets:\n",
    "        # check if a file exists in the data folder and if yes, do run_tests\n",
    "        try:\n",
    "            with open(f'data/{dataset}/{subset}/{subset}_data_500.csv', 'r') as csvfile:\n",
    "                print(f\"File '{subset}_data_500.csv' exists.\")\n",
    "                run_tests(dataset, subset, algorithms, sample_sizes)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"A file for the combination of {dataset} and {subset} does not exist.\")\n",
    "            pass\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
