{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Index, RangeIndex\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.datasets import IIDSimulation, DAG\n",
    "from castle.algorithms import GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FREQUENCY = 100\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Referred from:\n",
    "    - https://stackoverflow.com/questions/38469632/tensorflow-non-repeatable-results\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    try:\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def compute_h(w_adj):\n",
    "\n",
    "    d = w_adj.shape[0]\n",
    "    h = torch.trace(torch.matrix_exp(w_adj * w_adj)) - d\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "class Tensor(np.ndarray):\n",
    "    \"\"\"A subclass of numpy.ndarray.\n",
    "\n",
    "    This subclass has all attributes and methods of numpy.ndarray\n",
    "    with two additional, user-defined attributes: `index` and `columns`.\n",
    "\n",
    "    It can be used in the same way as a standard numpy.ndarray.\n",
    "    However, after performing any operations on the Tensor (e.g., slicing,\n",
    "    transposing, arithmetic, etc.), the user-defined attribute values of\n",
    "    `index` and `columns` will be lost and replaced with a numeric indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    object: array-like\n",
    "        Multiple list, ndarray, DataFrame\n",
    "    index : Index or array-like\n",
    "        Index to use for resulting tensor. Will default to RangeIndex if\n",
    "        no indexing information part of input data and no index provided.\n",
    "    columns : Index or array-like\n",
    "        Column labels to use for resulting tensor. Will default to\n",
    "        RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Create a Tensor from a list or numpy.ndarray.\n",
    "\n",
    "    >>> x = [[0, 3, 8, 1],\n",
    "    ...      [8, 4, 1, 9],\n",
    "    ...      [7, 3, 3, 7]]\n",
    "\n",
    "    Or\n",
    "\n",
    "    >>> x = np.random.randint(0, 10, size=12).reshape((3, 4))\n",
    "    >>> arr = Tensor(x)\n",
    "    >>> arr\n",
    "    Tensor([[0, 3, 8, 1],\n",
    "            [8, 4, 1, 9],\n",
    "            [7, 3, 3, 7]])\n",
    "    >>> arr.index\n",
    "    RangeIndex(start=0, stop=3, step=1)\n",
    "    >>> list(arr.index)\n",
    "    [0, 1, 2]\n",
    "    >>> arr.columns\n",
    "    RangeIndex(start=0, stop=4, step=1)\n",
    "    >>> list(arr.columns)\n",
    "    [0, 1, 2, 3]\n",
    "\n",
    "    `index` and `columns` can be set using kwargs.\n",
    "\n",
    "    >>> arr = Tensor(x, index=list('XYZ'), columns=list('ABCD'))\n",
    "    >>> arr\n",
    "    Tensor([[6, 1, 8, 9],\n",
    "            [1, 5, 2, 1],\n",
    "            [5, 9, 4, 5]])\n",
    "    >>> arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> arr.columns\n",
    "    Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "\n",
    "    Or a value can be assigned to `arr.index` or `arr.columns`,\n",
    "    but it must be an `Iterable`.\n",
    "\n",
    "    >>> arr.index = list('xyz')\n",
    "    >>> arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> arr.columns = list('abcd')\n",
    "    >>> arr.columns\n",
    "    Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "\n",
    "    A Tensor can also be created from a pandas.DataFrame.\n",
    "\n",
    "    >>> x = pd.DataFrame(np.random.randint(0, 10, size=12).reshape((3, 4)),\n",
    "    ...                  index=list('xyz'),\n",
    "    ...                  columns=list('abcd'))\n",
    "    >>> x\n",
    "       a  b  c  d\n",
    "    x  6  1  8  9\n",
    "    y  1  5  2  1\n",
    "    z  5  9  4  5\n",
    "    >>> arr = Tensor(x)\n",
    "    >>> arr\n",
    "    Tensor([[6, 1, 8, 9],\n",
    "            [1, 5, 2, 1],\n",
    "            [5, 9, 4, 5]])\n",
    "    >>> arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> arr.columns\n",
    "    Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "\n",
    "    It's possible to use any method of numpy.ndarray on the Tensor,\n",
    "    such as `sum`, `@`, etc.\n",
    "\n",
    "    >>> arr.sum(axis=0)\n",
    "    Tensor([15, 10, 12, 17])\n",
    "    >>> arr @ arr.T\n",
    "    Tensor([[ 74,  29,  40],\n",
    "            [ 29, 162, 134],\n",
    "            [ 40, 134, 116]])\n",
    "\n",
    "    If the Tensor is sliced, the values of `index` and `columns` will disappear,\n",
    "    and new values of type `RangeIndex` will be created.\n",
    "\n",
    "    >>> new_arr = arr[:, 1:3]\n",
    "    >>> new_arr\n",
    "    Tensor([[1, 8],\n",
    "            [5, 2],\n",
    "            [9, 4]])\n",
    "    >>> new_arr.index\n",
    "    RangeIndex(start=0, stop=3, step=1)\n",
    "    >>> new_arr.columns\n",
    "    RangeIndex(start=0, stop=2, step=1)\n",
    "\n",
    "    If you want to retain the values of `index` and `columns`,\n",
    "    you can reassign them.\n",
    "\n",
    "    >>> new_arr.index = arr.index[:]\n",
    "    >>> new_arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "\n",
    "    >>> new_arr.columns = arr.columns[1:3]\n",
    "    >>> new_arr.columns\n",
    "    Index(['b', 'c'], dtype='object')\n",
    "\n",
    "    We recommend performing slicing operations in the following way\n",
    "    to keep the `index` and `columns` values.\n",
    "\n",
    "    >>> new_arr = Tensor(array=arr[:, 1:3],\n",
    "    ...                  index=arr.index[:, 1:3],\n",
    "    ...                  columns=arr.columns[:, 1:3])\n",
    "    >>> new_arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> new_arr.columns\n",
    "    Index(['b', 'c'], dtype='object')\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, object=None, index=None, columns=None):\n",
    "\n",
    "        if object is None:\n",
    "            raise TypeError(\"Tensor() missing required argument 'object' (pos 0)\")\n",
    "        elif isinstance(object, list):\n",
    "            object = np.array(object)\n",
    "        elif isinstance(object, pd.DataFrame):\n",
    "            index = object.index\n",
    "            columns = object.columns\n",
    "            object = object.values\n",
    "        elif isinstance(object, (np.ndarray, cls)):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Type of the required argument 'object' must be array-like.\"\n",
    "            )\n",
    "        if index is None:\n",
    "            index = range(object.shape[0])\n",
    "        if columns is None:\n",
    "            columns = range(object.shape[1])\n",
    "        obj = np.asarray(object).view(cls)\n",
    "        obj.index = index\n",
    "        obj.columns = columns\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj):\n",
    "        if obj is None: return\n",
    "        if self.ndim == 0: return\n",
    "        elif self.ndim == 1:\n",
    "            self.columns = RangeIndex(0, 1, step=1, dtype=int)\n",
    "        else:\n",
    "            self.columns = RangeIndex(0, self.shape[1], step=1, dtype=int)\n",
    "        self.index = RangeIndex(0, self.shape[0], step=1, dtype=int)\n",
    "\n",
    "    @property\n",
    "    def index(self):\n",
    "        return self._index\n",
    "\n",
    "    @index.setter\n",
    "    def index(self, value):\n",
    "        assert isinstance(value, Iterable)\n",
    "        if len(list(value)) != self.shape[0]:\n",
    "            raise ValueError(\"Size of value is not equal to the shape[0].\")\n",
    "        self._index = Index(value)\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "\n",
    "    @columns.setter\n",
    "    def columns(self, value):\n",
    "        assert isinstance(value, Iterable)\n",
    "        if (self.ndim > 1 and len(list(value)) != self.shape[1]):\n",
    "            raise ValueError(\"Size of value is not equal to the shape[1].\")\n",
    "        self._columns = Index(value)\n",
    "\n",
    "\n",
    "class ALTrainer(object):\n",
    "\n",
    "    def __init__(self, n, d, model, lr, init_iter, alpha, beta, rho, rho_thresh,\n",
    "                 h_thresh, l1_penalty, gamma, early_stopping,\n",
    "                 early_stopping_thresh, seed, device=None):\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.init_iter = init_iter\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta  # rho_multiply\n",
    "        self.rho = rho\n",
    "        self.rho_thresh = rho_thresh\n",
    "        self.h_thresh = h_thresh  # 1e-8\n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.gamma = gamma\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_thresh = early_stopping_thresh\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                          lr=self.lr)\n",
    "\n",
    "    def train(self, x, epochs, update_freq):\n",
    "\n",
    "        alpha, beta, rho = self.alpha, self.beta, self.rho\n",
    "        h, h_new = np.inf, np.inf\n",
    "        prev_w_est, prev_mse = None, np.inf\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logging.info(f'Current epoch: {epoch}==================')\n",
    "            while rho < self.rho_thresh:\n",
    "                mse_new, h_new, w_new = self.train_step(x,\n",
    "                                                        update_freq,\n",
    "                                                        alpha,\n",
    "                                                        rho)\n",
    "                if h_new > self.gamma * h:\n",
    "                    rho *= self.beta\n",
    "                else:\n",
    "                    break\n",
    "            logging.info(f'Current        h: {h_new}')\n",
    "\n",
    "            if self.early_stopping:\n",
    "                if (mse_new / prev_mse > self.early_stopping_thresh\n",
    "                        and h_new <= 1e-7):\n",
    "                    return prev_w_est\n",
    "                else:\n",
    "                    prev_w_est = w_new\n",
    "                    prev_mse = mse_new\n",
    "\n",
    "            # update rules\n",
    "            w_est, h = w_new, h_new\n",
    "            alpha += rho * h_new.detach().cpu()\n",
    "\n",
    "            if h <= self.h_thresh and epoch > self.init_iter:\n",
    "                break\n",
    "\n",
    "        return w_est\n",
    "\n",
    "\n",
    "    def train_step(self, x, update_freq, alpha, rho):\n",
    "\n",
    "        curr_mse, curr_h, w_adj = None, None, None\n",
    "        for _ in range(update_freq):\n",
    "            torch.manual_seed(self.seed)\n",
    "            curr_mse, w_adj = self.model(x)\n",
    "            curr_h = compute_h(w_adj)\n",
    "            loss = ((0.5 / self.n) * curr_mse\n",
    "                    + self.l1_penalty * torch.norm(w_adj, p=1)\n",
    "                    + alpha * curr_h + 0.5 * rho * curr_h * curr_h)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if _ % LOG_FREQUENCY == 0:\n",
    "                logging.info(f'Current loss in step {_}: {loss.detach()}')\n",
    "\n",
    "        return curr_mse, curr_h, w_adj\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward neural networks----MLP\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, layers, units, output_dim,\n",
    "                 activation=None, device=None) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        # self.desc = desc\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = layers\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.device = device\n",
    "\n",
    "        mlp = []\n",
    "        for i in range(layers):\n",
    "            input_size = units\n",
    "            if i == 0:\n",
    "                input_size = input_dim\n",
    "            weight = nn.Linear(in_features=input_size,\n",
    "                               out_features=self.units,\n",
    "                               bias=True,\n",
    "                               device=self.device)\n",
    "            mlp.append(weight)\n",
    "            if activation is not None:\n",
    "                mlp.append(activation)\n",
    "        out_layer = nn.Linear(in_features=self.units,\n",
    "                              out_features=self.output_dim,\n",
    "                              bias=True,\n",
    "                              device=self.device)\n",
    "        mlp.append(out_layer)\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "\n",
    "        x_ = x.reshape(-1, self.input_dim)\n",
    "        output = self.mlp(x_)\n",
    "\n",
    "        return output.reshape(x.shape[0], -1, self.output_dim)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d, input_dim, hidden_layers=3, hidden_dim=16,\n",
    "                 activation=nn.ReLU(), device=None):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.d = d\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = MLP(input_dim=self.input_dim,\n",
    "                           layers=self.hidden_layers,\n",
    "                           units=self.hidden_dim,\n",
    "                           output_dim=self.hidden_dim,\n",
    "                           activation=self.activation,\n",
    "                           device=self.device)\n",
    "        self.decoder = MLP(input_dim=self.hidden_dim,\n",
    "                           layers=self.hidden_layers,\n",
    "                           units=self.hidden_dim,\n",
    "                           output_dim=self.input_dim,\n",
    "                           activation=self.activation,\n",
    "                           device=self.device)\n",
    "\n",
    "        w = torch.nn.init.uniform_(torch.empty(self.d, self.d,),\n",
    "                                   a=-0.1, b=0.1)\n",
    "        self.w = torch.nn.Parameter(w.to(device=self.device))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.w_adj = self._preprocess_graph(self.w)\n",
    "\n",
    "        out = self.encoder(x)\n",
    "        out = torch.einsum('ijk,jl->ilk', out, self.w_adj)\n",
    "        x_est = torch.sigmoid(self.decoder(out))\n",
    "\n",
    "        # mse_loss = torch.square(torch.norm(x - x_est, p=2))\n",
    "        mse_loss = F.binary_cross_entropy(x, x_est)\n",
    "\n",
    "\n",
    "        return mse_loss, self.w_adj\n",
    "\n",
    "    def _preprocess_graph(self, w_adj):\n",
    "\n",
    "        return (1. - torch.eye(w_adj.shape[0], device=self.device)) * w_adj\n",
    "\n",
    "\n",
    "class GAE:\n",
    "    \"\"\"\n",
    "    GAE Algorithm.\n",
    "    A gradient-based algorithm using graph autoencoder to model non-linear\n",
    "    causal relationships.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim: int, default: 1\n",
    "        dimension of vector for x\n",
    "    hidden_layers: int, default: 1\n",
    "        number of hidden layers for encoder and decoder\n",
    "    hidden_dim: int, default: 4\n",
    "        hidden size for mlp layer\n",
    "    activation: callable, default: nn.LeakyReLU(0.05)\n",
    "        nonlinear functional\n",
    "    epochs: int, default: 10\n",
    "        Number of iterations for optimization problem\n",
    "    update_freq: int, default: 3000\n",
    "        Number of steps for each iteration\n",
    "    init_iter: int, default: 3\n",
    "        Initial iteration to disallow early stopping\n",
    "    lr: float, default: 1e-3\n",
    "        learning rate\n",
    "    alpha: float, default: 0.0\n",
    "        Lagrange multiplier\n",
    "    beta: float, default: 2.0\n",
    "        Multiplication to amplify rho each time\n",
    "    init_rho: float, default: 1.0\n",
    "        Initial value for rho\n",
    "    rho_thresh: float, default: 1e30\n",
    "        Threshold for rho\n",
    "    gamma: float, default: 0.25\n",
    "        Threshold for h\n",
    "    penalty_lambda: float, default: 0.0\n",
    "        L1 penalty for sparse graph. Set to 0.0 to disable\n",
    "    h_thresh: float, default: 1e-8\n",
    "        Tolerance of optimization problem\n",
    "    graph_thresh: float, default: 0.3\n",
    "        Threshold to filter out small values in graph\n",
    "    early_stopping: bool, default: False\n",
    "        Whether to use early stopping\n",
    "    early_stopping_thresh: float, default: 1.0\n",
    "        Threshold ratio for early stopping\n",
    "    seed: int, default: 1230\n",
    "        Reproducibility, must be int\n",
    "    device_type: str, default: 'cpu'\n",
    "        'cpu' or 'gpu'\n",
    "    device_ids: int or str, default '0'\n",
    "        CUDA devices, it's effective when ``use_gpu`` is True.\n",
    "        For single-device modules, ``device_ids`` can be int or str,\n",
    "        e.g. 0 or '0', For multi-device modules, ``device_ids`` must be str,\n",
    "        format like '0, 1'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim=1,\n",
    "                 hidden_layers=1,\n",
    "                 hidden_dim=4,\n",
    "                 activation=torch.nn.LeakyReLU(0.05),\n",
    "                 epochs=10,\n",
    "                 update_freq=3000,\n",
    "                 init_iter=3,\n",
    "                 lr=1e-3,\n",
    "                 alpha=0.0,\n",
    "                 beta=2.0,\n",
    "                 init_rho=1.0,\n",
    "                 rho_thresh=1e30,\n",
    "                 gamma=0.25,\n",
    "                 penalty_lambda=0.0,\n",
    "                 h_thresh=1e-8,\n",
    "                 graph_thresh=0.3,\n",
    "                 early_stopping=False,\n",
    "                 early_stopping_thresh=1.0,\n",
    "                 seed=1230,\n",
    "                 device_type='cpu',\n",
    "                 device_ids='0'):\n",
    "\n",
    "        super(GAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.epochs = epochs\n",
    "        self.update_freq = update_freq\n",
    "        self.init_iter = init_iter\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.init_rho = init_rho\n",
    "        self.rho_thresh = rho_thresh\n",
    "        self.gamma = gamma\n",
    "        self.penalty_lambda = penalty_lambda\n",
    "        self.h_thresh = h_thresh\n",
    "        self.graph_thresh = graph_thresh\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_thresh = early_stopping_thresh\n",
    "        self.seed = seed\n",
    "        self.device_type = device_type\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            logging.info('GPU is available.')\n",
    "        else:\n",
    "            logging.info('GPU is unavailable.')\n",
    "            if self.device_type == 'gpu':\n",
    "                raise ValueError(\"GPU is unavailable, \"\n",
    "                                 \"please set device_type = 'cpu'.\")\n",
    "        if self.device_type == 'gpu':\n",
    "            if self.device_ids:\n",
    "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
    "            device = torch.device('cuda')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.device = device\n",
    "\n",
    "    def learn(self, data, columns=None, **kwargs):\n",
    "\n",
    "        x = torch.from_numpy(data)\n",
    "\n",
    "        self.n, self.d = x.shape[:2]\n",
    "        if x.ndim == 2:\n",
    "            x = x.reshape((self.n, self.d, 1))\n",
    "            self.input_dim = 1\n",
    "        elif x.ndim == 3:\n",
    "            self.input_dim = x.shape[2]\n",
    "\n",
    "        w_est = self._gae(x).detach().cpu().numpy()\n",
    "\n",
    "        self.weight_causal_matrix = Tensor(w_est,\n",
    "                                           index=columns,\n",
    "                                           columns=columns)\n",
    "        causal_matrix = (abs(w_est) > self.graph_thresh).astype(int)\n",
    "        self.causal_matrix = Tensor(causal_matrix,\n",
    "                                    index=columns,\n",
    "                                    columns=columns)\n",
    "\n",
    "    def _gae(self, x):\n",
    "\n",
    "        set_seed(self.seed)\n",
    "        model = AutoEncoder(d=self.d,\n",
    "                            input_dim=self.input_dim,\n",
    "                            hidden_layers=self.hidden_layers,\n",
    "                            hidden_dim=self.hidden_dim,\n",
    "                            activation=self.activation,\n",
    "                            device=self.device,\n",
    "                            )\n",
    "        trainer = ALTrainer(n=self.n,\n",
    "                            d=self.d,\n",
    "                            model=model,\n",
    "                            lr=self.lr,\n",
    "                            init_iter=self.init_iter,\n",
    "                            alpha=self.alpha,\n",
    "                            beta=self.beta,\n",
    "                            rho=self.init_rho,\n",
    "                            l1_penalty=self.penalty_lambda,\n",
    "                            rho_thresh=self.rho_thresh,\n",
    "                            h_thresh=self.h_thresh,  # 1e-8\n",
    "                            early_stopping=self.early_stopping,\n",
    "                            early_stopping_thresh=self.early_stopping_thresh,\n",
    "                            gamma=self.gamma,\n",
    "                            seed=self.seed,\n",
    "                            device=self.device)\n",
    "        w_est = trainer.train(x=x,\n",
    "                              epochs=self.epochs,\n",
    "                              update_freq=self.update_freq)\n",
    "        w_est = w_est / torch.max(abs(w_est))\n",
    "\n",
    "        return w_est\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG.scale_free(n_nodes=43, n_edges=40)\n",
    "data = IIDSimulation(W=dag, n=1000, method='linear', sem_type='logistic')\n",
    "\n",
    "model = GAE(epochs=3)\n",
    "model.learn(data.X)\n",
    "\n",
    "GraphDAG(model.causal_matrix, data.B)\n",
    "metrics = MetricsDAG(model.causal_matrix, data.B)\n",
    "print(metrics.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
