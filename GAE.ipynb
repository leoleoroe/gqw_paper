{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Index, RangeIndex\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.datasets import IIDSimulation, DAG\n",
    "from castle.algorithms import GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FREQUENCY = 100\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Referred from:\n",
    "    - https://stackoverflow.com/questions/38469632/tensorflow-non-repeatable-results\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    try:\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def compute_h(w_adj):\n",
    "\n",
    "    d = w_adj.shape[0]\n",
    "    h = torch.trace(torch.matrix_exp(w_adj * w_adj)) - d\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "class Tensor(np.ndarray):\n",
    "    \"\"\"A subclass of numpy.ndarray.\n",
    "\n",
    "    This subclass has all attributes and methods of numpy.ndarray\n",
    "    with two additional, user-defined attributes: `index` and `columns`.\n",
    "\n",
    "    It can be used in the same way as a standard numpy.ndarray.\n",
    "    However, after performing any operations on the Tensor (e.g., slicing,\n",
    "    transposing, arithmetic, etc.), the user-defined attribute values of\n",
    "    `index` and `columns` will be lost and replaced with a numeric indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    object: array-like\n",
    "        Multiple list, ndarray, DataFrame\n",
    "    index : Index or array-like\n",
    "        Index to use for resulting tensor. Will default to RangeIndex if\n",
    "        no indexing information part of input data and no index provided.\n",
    "    columns : Index or array-like\n",
    "        Column labels to use for resulting tensor. Will default to\n",
    "        RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Create a Tensor from a list or numpy.ndarray.\n",
    "\n",
    "    >>> x = [[0, 3, 8, 1],\n",
    "    ...      [8, 4, 1, 9],\n",
    "    ...      [7, 3, 3, 7]]\n",
    "\n",
    "    Or\n",
    "\n",
    "    >>> x = np.random.randint(0, 10, size=12).reshape((3, 4))\n",
    "    >>> arr = Tensor(x)\n",
    "    >>> arr\n",
    "    Tensor([[0, 3, 8, 1],\n",
    "            [8, 4, 1, 9],\n",
    "            [7, 3, 3, 7]])\n",
    "    >>> arr.index\n",
    "    RangeIndex(start=0, stop=3, step=1)\n",
    "    >>> list(arr.index)\n",
    "    [0, 1, 2]\n",
    "    >>> arr.columns\n",
    "    RangeIndex(start=0, stop=4, step=1)\n",
    "    >>> list(arr.columns)\n",
    "    [0, 1, 2, 3]\n",
    "\n",
    "    `index` and `columns` can be set using kwargs.\n",
    "\n",
    "    >>> arr = Tensor(x, index=list('XYZ'), columns=list('ABCD'))\n",
    "    >>> arr\n",
    "    Tensor([[6, 1, 8, 9],\n",
    "            [1, 5, 2, 1],\n",
    "            [5, 9, 4, 5]])\n",
    "    >>> arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> arr.columns\n",
    "    Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "\n",
    "    Or a value can be assigned to `arr.index` or `arr.columns`,\n",
    "    but it must be an `Iterable`.\n",
    "\n",
    "    >>> arr.index = list('xyz')\n",
    "    >>> arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> arr.columns = list('abcd')\n",
    "    >>> arr.columns\n",
    "    Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "\n",
    "    A Tensor can also be created from a pandas.DataFrame.\n",
    "\n",
    "    >>> x = pd.DataFrame(np.random.randint(0, 10, size=12).reshape((3, 4)),\n",
    "    ...                  index=list('xyz'),\n",
    "    ...                  columns=list('abcd'))\n",
    "    >>> x\n",
    "       a  b  c  d\n",
    "    x  6  1  8  9\n",
    "    y  1  5  2  1\n",
    "    z  5  9  4  5\n",
    "    >>> arr = Tensor(x)\n",
    "    >>> arr\n",
    "    Tensor([[6, 1, 8, 9],\n",
    "            [1, 5, 2, 1],\n",
    "            [5, 9, 4, 5]])\n",
    "    >>> arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> arr.columns\n",
    "    Index(['a', 'b', 'c', 'd'], dtype='object')\n",
    "\n",
    "    It's possible to use any method of numpy.ndarray on the Tensor,\n",
    "    such as `sum`, `@`, etc.\n",
    "\n",
    "    >>> arr.sum(axis=0)\n",
    "    Tensor([15, 10, 12, 17])\n",
    "    >>> arr @ arr.T\n",
    "    Tensor([[ 74,  29,  40],\n",
    "            [ 29, 162, 134],\n",
    "            [ 40, 134, 116]])\n",
    "\n",
    "    If the Tensor is sliced, the values of `index` and `columns` will disappear,\n",
    "    and new values of type `RangeIndex` will be created.\n",
    "\n",
    "    >>> new_arr = arr[:, 1:3]\n",
    "    >>> new_arr\n",
    "    Tensor([[1, 8],\n",
    "            [5, 2],\n",
    "            [9, 4]])\n",
    "    >>> new_arr.index\n",
    "    RangeIndex(start=0, stop=3, step=1)\n",
    "    >>> new_arr.columns\n",
    "    RangeIndex(start=0, stop=2, step=1)\n",
    "\n",
    "    If you want to retain the values of `index` and `columns`,\n",
    "    you can reassign them.\n",
    "\n",
    "    >>> new_arr.index = arr.index[:]\n",
    "    >>> new_arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "\n",
    "    >>> new_arr.columns = arr.columns[1:3]\n",
    "    >>> new_arr.columns\n",
    "    Index(['b', 'c'], dtype='object')\n",
    "\n",
    "    We recommend performing slicing operations in the following way\n",
    "    to keep the `index` and `columns` values.\n",
    "\n",
    "    >>> new_arr = Tensor(array=arr[:, 1:3],\n",
    "    ...                  index=arr.index[:, 1:3],\n",
    "    ...                  columns=arr.columns[:, 1:3])\n",
    "    >>> new_arr.index\n",
    "    Index(['x', 'y', 'z'], dtype='object')\n",
    "    >>> new_arr.columns\n",
    "    Index(['b', 'c'], dtype='object')\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, object=None, index=None, columns=None):\n",
    "\n",
    "        if object is None:\n",
    "            raise TypeError(\"Tensor() missing required argument 'object' (pos 0)\")\n",
    "        elif isinstance(object, list):\n",
    "            object = np.array(object)\n",
    "        elif isinstance(object, pd.DataFrame):\n",
    "            index = object.index\n",
    "            columns = object.columns\n",
    "            object = object.values\n",
    "        elif isinstance(object, (np.ndarray, cls)):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Type of the required argument 'object' must be array-like.\"\n",
    "            )\n",
    "        if index is None:\n",
    "            index = range(object.shape[0])\n",
    "        if columns is None:\n",
    "            columns = range(object.shape[1])\n",
    "        obj = np.asarray(object).view(cls)\n",
    "        obj.index = index\n",
    "        obj.columns = columns\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj):\n",
    "        if obj is None: return\n",
    "        if self.ndim == 0: return\n",
    "        elif self.ndim == 1:\n",
    "            self.columns = RangeIndex(0, 1, step=1, dtype=int)\n",
    "        else:\n",
    "            self.columns = RangeIndex(0, self.shape[1], step=1, dtype=int)\n",
    "        self.index = RangeIndex(0, self.shape[0], step=1, dtype=int)\n",
    "\n",
    "    @property\n",
    "    def index(self):\n",
    "        return self._index\n",
    "\n",
    "    @index.setter\n",
    "    def index(self, value):\n",
    "        assert isinstance(value, Iterable)\n",
    "        if len(list(value)) != self.shape[0]:\n",
    "            raise ValueError(\"Size of value is not equal to the shape[0].\")\n",
    "        self._index = Index(value)\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "\n",
    "    @columns.setter\n",
    "    def columns(self, value):\n",
    "        assert isinstance(value, Iterable)\n",
    "        if (self.ndim > 1 and len(list(value)) != self.shape[1]):\n",
    "            raise ValueError(\"Size of value is not equal to the shape[1].\")\n",
    "        self._columns = Index(value)\n",
    "\n",
    "\n",
    "class ALTrainer(object):\n",
    "\n",
    "    def __init__(self, n, d, model, lr, init_iter, alpha, beta, rho, rho_thresh,\n",
    "                 h_thresh, l1_penalty, gamma, early_stopping,\n",
    "                 early_stopping_thresh, seed, device=None):\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.init_iter = init_iter\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta  # rho_multiply\n",
    "        self.rho = rho\n",
    "        self.rho_thresh = rho_thresh\n",
    "        self.h_thresh = h_thresh  # 1e-8\n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.gamma = gamma\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_thresh = early_stopping_thresh\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                          lr=self.lr)\n",
    "\n",
    "    def train(self, x, epochs, update_freq):\n",
    "\n",
    "        alpha, beta, rho = self.alpha, self.beta, self.rho\n",
    "        h, h_new = np.inf, np.inf\n",
    "        prev_w_est, prev_mse = None, np.inf\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logging.info(f'Current epoch: {epoch}==================')\n",
    "            while rho < self.rho_thresh:\n",
    "                mse_new, h_new, w_new = self.train_step(x,\n",
    "                                                        update_freq,\n",
    "                                                        alpha,\n",
    "                                                        rho)\n",
    "                if h_new > self.gamma * h:\n",
    "                    rho *= self.beta\n",
    "                else:\n",
    "                    break\n",
    "            logging.info(f'Current        h: {h_new}')\n",
    "\n",
    "            if self.early_stopping:\n",
    "                if (mse_new / prev_mse > self.early_stopping_thresh\n",
    "                        and h_new <= 1e-7):\n",
    "                    return prev_w_est\n",
    "                else:\n",
    "                    prev_w_est = w_new\n",
    "                    prev_mse = mse_new\n",
    "\n",
    "            # update rules\n",
    "            w_est, h = w_new, h_new\n",
    "            alpha += rho * h_new.detach().cpu()\n",
    "\n",
    "            if h <= self.h_thresh and epoch > self.init_iter:\n",
    "                break\n",
    "\n",
    "        return w_est\n",
    "\n",
    "\n",
    "    def train_step(self, x, update_freq, alpha, rho):\n",
    "\n",
    "        curr_mse, curr_h, w_adj = None, None, None\n",
    "        for _ in range(update_freq):\n",
    "            torch.manual_seed(self.seed)\n",
    "            curr_mse, w_adj = self.model(x)\n",
    "            curr_h = compute_h(w_adj)\n",
    "            loss = ((0.5 / self.n) * curr_mse\n",
    "                    + self.l1_penalty * torch.norm(w_adj, p=1)\n",
    "                    + alpha * curr_h + 0.5 * rho * curr_h * curr_h)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if _ % LOG_FREQUENCY == 0:\n",
    "                logging.info(f'Current loss in step {_}: {loss.detach()}')\n",
    "\n",
    "        return curr_mse, curr_h, w_adj\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward neural networks----MLP\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, layers, units, output_dim,\n",
    "                 activation=None, device=None) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        # self.desc = desc\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = layers\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.device = device\n",
    "\n",
    "        mlp = []\n",
    "        for i in range(layers):\n",
    "            input_size = units\n",
    "            if i == 0:\n",
    "                input_size = input_dim\n",
    "            weight = nn.Linear(in_features=input_size,\n",
    "                               out_features=self.units,\n",
    "                               bias=True,\n",
    "                               device=self.device)\n",
    "            mlp.append(weight)\n",
    "            if activation is not None:\n",
    "                mlp.append(activation)\n",
    "        out_layer = nn.Linear(in_features=self.units,\n",
    "                              out_features=self.output_dim,\n",
    "                              bias=True,\n",
    "                              device=self.device)\n",
    "        mlp.append(out_layer)\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "\n",
    "        x_ = x.reshape(-1, self.input_dim)\n",
    "        output = self.mlp(x_)\n",
    "\n",
    "        return output.reshape(x.shape[0], -1, self.output_dim)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d, input_dim, hidden_layers=3, hidden_dim=16,\n",
    "                 activation=nn.ReLU(), device=None):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.d = d\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = MLP(input_dim=self.input_dim,\n",
    "                           layers=self.hidden_layers,\n",
    "                           units=self.hidden_dim,\n",
    "                           output_dim=self.hidden_dim,\n",
    "                           activation=self.activation,\n",
    "                           device=self.device)\n",
    "        self.decoder = MLP(input_dim=self.hidden_dim,\n",
    "                           layers=self.hidden_layers,\n",
    "                           units=self.hidden_dim,\n",
    "                           output_dim=self.input_dim,\n",
    "                           activation=self.activation,\n",
    "                           device=self.device)\n",
    "\n",
    "        w = torch.nn.init.uniform_(torch.empty(self.d, self.d,),\n",
    "                                   a=-0.1, b=0.1)\n",
    "        self.w = torch.nn.Parameter(w.to(device=self.device))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.w_adj = self._preprocess_graph(self.w)\n",
    "\n",
    "        out = self.encoder(x)\n",
    "        out = torch.einsum('ijk,jl->ilk', out, self.w_adj)\n",
    "        x_est = torch.sigmoid(self.decoder(out))\n",
    "\n",
    "        # mse_loss = torch.square(torch.norm(x - x_est, p=2))\n",
    "        mse_loss = F.binary_cross_entropy(x, x_est)\n",
    "\n",
    "\n",
    "        return mse_loss, self.w_adj\n",
    "\n",
    "    def _preprocess_graph(self, w_adj):\n",
    "\n",
    "        return (1. - torch.eye(w_adj.shape[0], device=self.device)) * w_adj\n",
    "\n",
    "\n",
    "class GAE:\n",
    "    \"\"\"\n",
    "    GAE Algorithm.\n",
    "    A gradient-based algorithm using graph autoencoder to model non-linear\n",
    "    causal relationships.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim: int, default: 1\n",
    "        dimension of vector for x\n",
    "    hidden_layers: int, default: 1\n",
    "        number of hidden layers for encoder and decoder\n",
    "    hidden_dim: int, default: 4\n",
    "        hidden size for mlp layer\n",
    "    activation: callable, default: nn.LeakyReLU(0.05)\n",
    "        nonlinear functional\n",
    "    epochs: int, default: 10\n",
    "        Number of iterations for optimization problem\n",
    "    update_freq: int, default: 3000\n",
    "        Number of steps for each iteration\n",
    "    init_iter: int, default: 3\n",
    "        Initial iteration to disallow early stopping\n",
    "    lr: float, default: 1e-3\n",
    "        learning rate\n",
    "    alpha: float, default: 0.0\n",
    "        Lagrange multiplier\n",
    "    beta: float, default: 2.0\n",
    "        Multiplication to amplify rho each time\n",
    "    init_rho: float, default: 1.0\n",
    "        Initial value for rho\n",
    "    rho_thresh: float, default: 1e30\n",
    "        Threshold for rho\n",
    "    gamma: float, default: 0.25\n",
    "        Threshold for h\n",
    "    penalty_lambda: float, default: 0.0\n",
    "        L1 penalty for sparse graph. Set to 0.0 to disable\n",
    "    h_thresh: float, default: 1e-8\n",
    "        Tolerance of optimization problem\n",
    "    graph_thresh: float, default: 0.3\n",
    "        Threshold to filter out small values in graph\n",
    "    early_stopping: bool, default: False\n",
    "        Whether to use early stopping\n",
    "    early_stopping_thresh: float, default: 1.0\n",
    "        Threshold ratio for early stopping\n",
    "    seed: int, default: 1230\n",
    "        Reproducibility, must be int\n",
    "    device_type: str, default: 'cpu'\n",
    "        'cpu' or 'gpu'\n",
    "    device_ids: int or str, default '0'\n",
    "        CUDA devices, it's effective when ``use_gpu`` is True.\n",
    "        For single-device modules, ``device_ids`` can be int or str,\n",
    "        e.g. 0 or '0', For multi-device modules, ``device_ids`` must be str,\n",
    "        format like '0, 1'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim=1,\n",
    "                 hidden_layers=1,\n",
    "                 hidden_dim=4,\n",
    "                 activation=torch.nn.LeakyReLU(0.05),\n",
    "                 epochs=10,\n",
    "                 update_freq=3000,\n",
    "                 init_iter=3,\n",
    "                 lr=1e-3,\n",
    "                 alpha=0.0,\n",
    "                 beta=2.0,\n",
    "                 init_rho=1.0,\n",
    "                 rho_thresh=1e30,\n",
    "                 gamma=0.25,\n",
    "                 penalty_lambda=0.0,\n",
    "                 h_thresh=1e-8,\n",
    "                 graph_thresh=0.3,\n",
    "                 early_stopping=False,\n",
    "                 early_stopping_thresh=1.0,\n",
    "                 seed=1230,\n",
    "                 device_type='cpu',\n",
    "                 device_ids='0'):\n",
    "\n",
    "        super(GAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.epochs = epochs\n",
    "        self.update_freq = update_freq\n",
    "        self.init_iter = init_iter\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.init_rho = init_rho\n",
    "        self.rho_thresh = rho_thresh\n",
    "        self.gamma = gamma\n",
    "        self.penalty_lambda = penalty_lambda\n",
    "        self.h_thresh = h_thresh\n",
    "        self.graph_thresh = graph_thresh\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_thresh = early_stopping_thresh\n",
    "        self.seed = seed\n",
    "        self.device_type = device_type\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            logging.info('GPU is available.')\n",
    "        else:\n",
    "            logging.info('GPU is unavailable.')\n",
    "            if self.device_type == 'gpu':\n",
    "                raise ValueError(\"GPU is unavailable, \"\n",
    "                                 \"please set device_type = 'cpu'.\")\n",
    "        if self.device_type == 'gpu':\n",
    "            if self.device_ids:\n",
    "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
    "            device = torch.device('cuda')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.device = device\n",
    "\n",
    "    def learn(self, data, columns=None, **kwargs):\n",
    "\n",
    "        x = torch.from_numpy(data)\n",
    "\n",
    "        self.n, self.d = x.shape[:2]\n",
    "        if x.ndim == 2:\n",
    "            x = x.reshape((self.n, self.d, 1))\n",
    "            self.input_dim = 1\n",
    "        elif x.ndim == 3:\n",
    "            self.input_dim = x.shape[2]\n",
    "\n",
    "        w_est = self._gae(x).detach().cpu().numpy()\n",
    "\n",
    "        self.weight_causal_matrix = Tensor(w_est,\n",
    "                                           index=columns,\n",
    "                                           columns=columns)\n",
    "        causal_matrix = (abs(w_est) > self.graph_thresh).astype(int)\n",
    "        self.causal_matrix = Tensor(causal_matrix,\n",
    "                                    index=columns,\n",
    "                                    columns=columns)\n",
    "\n",
    "    def _gae(self, x):\n",
    "\n",
    "        set_seed(self.seed)\n",
    "        model = AutoEncoder(d=self.d,\n",
    "                            input_dim=self.input_dim,\n",
    "                            hidden_layers=self.hidden_layers,\n",
    "                            hidden_dim=self.hidden_dim,\n",
    "                            activation=self.activation,\n",
    "                            device=self.device,\n",
    "                            )\n",
    "        trainer = ALTrainer(n=self.n,\n",
    "                            d=self.d,\n",
    "                            model=model,\n",
    "                            lr=self.lr,\n",
    "                            init_iter=self.init_iter,\n",
    "                            alpha=self.alpha,\n",
    "                            beta=self.beta,\n",
    "                            rho=self.init_rho,\n",
    "                            l1_penalty=self.penalty_lambda,\n",
    "                            rho_thresh=self.rho_thresh,\n",
    "                            h_thresh=self.h_thresh,  # 1e-8\n",
    "                            early_stopping=self.early_stopping,\n",
    "                            early_stopping_thresh=self.early_stopping_thresh,\n",
    "                            gamma=self.gamma,\n",
    "                            seed=self.seed,\n",
    "                            device=self.device)\n",
    "        w_est = trainer.train(x=x,\n",
    "                              epochs=self.epochs,\n",
    "                              update_freq=self.update_freq)\n",
    "        w_est = w_est / torch.max(abs(w_est))\n",
    "\n",
    "        return w_est\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 16:34:08,520 - c:\\Users\\q619374\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
      "2024-06-13 16:34:08,521 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:499] - INFO: GPU is unavailable.\n",
      "2024-06-13 16:34:08,524 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:252] - INFO: Current epoch: 1==================\n",
      "2024-06-13 16:34:08,535 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 0: 4.198097088325455\n",
      "2024-06-13 16:34:08,863 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 100: 2.837418990904954\n",
      "2024-06-13 16:34:09,264 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 200: 2.1213843055107082\n",
      "2024-06-13 16:34:09,634 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 300: 0.5808292596415984\n",
      "2024-06-13 16:34:10,003 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 400: 0.563305128286654\n",
      "2024-06-13 16:34:10,373 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 500: 0.5621992227337894\n",
      "2024-06-13 16:34:10,707 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 600: 0.5610543499469598\n",
      "2024-06-13 16:34:11,049 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 700: 0.5597531304338332\n",
      "2024-06-13 16:34:11,401 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 800: 0.558248617206943\n",
      "2024-06-13 16:34:11,739 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 900: 0.556488672233587\n",
      "2024-06-13 16:34:12,088 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1000: 0.5544193816894164\n",
      "2024-06-13 16:34:12,433 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1100: 0.5519887089840809\n",
      "2024-06-13 16:34:12,864 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1200: 0.5491533233543564\n",
      "2024-06-13 16:34:13,230 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1300: 0.545894888935115\n",
      "2024-06-13 16:34:13,564 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1400: 0.5422570191105549\n",
      "2024-06-13 16:34:13,900 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1500: 0.5384094914565806\n",
      "2024-06-13 16:34:14,234 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1600: 0.5346996947116887\n",
      "2024-06-13 16:34:14,584 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1700: 0.5315652609165046\n",
      "2024-06-13 16:34:14,950 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1800: 0.529260010122594\n",
      "2024-06-13 16:34:15,316 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1900: 0.5276994042104127\n",
      "2024-06-13 16:34:15,647 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2000: 0.5266440481055479\n",
      "2024-06-13 16:34:15,977 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2100: 0.5259010330448978\n",
      "2024-06-13 16:34:16,314 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2200: 0.5253548331199798\n",
      "2024-06-13 16:34:16,661 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2300: 0.5249387207937996\n",
      "2024-06-13 16:34:16,997 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2400: 0.5246130418419791\n",
      "2024-06-13 16:34:17,338 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2500: 0.5243531945916641\n",
      "2024-06-13 16:34:17,817 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2600: 0.5241430427392649\n",
      "2024-06-13 16:34:18,198 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2700: 0.5239713745602815\n",
      "2024-06-13 16:34:18,616 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2800: 0.5238300157647888\n",
      "2024-06-13 16:34:19,063 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2900: 0.5237127954193271\n",
      "2024-06-13 16:34:19,404 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:262] - INFO: Current        h: 0.02902249625406128\n",
      "2024-06-13 16:34:19,404 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:252] - INFO: Current epoch: 2==================\n",
      "2024-06-13 16:34:19,404 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 0: 0.5244568276772578\n",
      "2024-06-13 16:34:19,776 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 100: 0.5242366467949529\n",
      "2024-06-13 16:34:20,124 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 200: 0.5240703688596043\n",
      "2024-06-13 16:34:20,503 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 300: 0.5239339555071062\n",
      "2024-06-13 16:34:20,937 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 400: 0.523818808905723\n",
      "2024-06-13 16:34:21,371 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 500: 0.523720503424598\n",
      "2024-06-13 16:34:21,930 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 600: 0.5236358452361529\n",
      "2024-06-13 16:34:22,349 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 700: 0.5235623330688528\n",
      "2024-06-13 16:34:22,847 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 800: 0.5234979740121274\n",
      "2024-06-13 16:34:23,297 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 900: 0.5234411702277993\n",
      "2024-06-13 16:34:23,724 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1000: 0.5233906330527791\n",
      "2024-06-13 16:34:24,167 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1100: 0.52334531645275\n",
      "2024-06-13 16:34:24,596 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1200: 0.523304365594547\n",
      "2024-06-13 16:34:25,050 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1300: 0.5232670769764991\n",
      "2024-06-13 16:34:25,546 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1400: 0.5232328673285772\n",
      "2024-06-13 16:34:25,902 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1500: 0.5232012492175336\n",
      "2024-06-13 16:34:26,297 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1600: 0.5231718118274609\n",
      "2024-06-13 16:34:26,720 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1700: 0.5231442057581693\n",
      "2024-06-13 16:34:27,091 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1800: 0.5231181309488481\n",
      "2024-06-13 16:34:27,565 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1900: 0.5230933270326023\n",
      "2024-06-13 16:34:27,960 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2000: 0.5230695655808743\n",
      "2024-06-13 16:34:28,398 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2100: 0.5230466438171119\n",
      "2024-06-13 16:34:28,731 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2200: 0.5230243794734901\n",
      "2024-06-13 16:34:29,046 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2300: 0.5230026065381028\n",
      "2024-06-13 16:34:29,364 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2400: 0.5229811716972154\n",
      "2024-06-13 16:34:29,667 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2500: 0.5229599313215795\n",
      "2024-06-13 16:34:30,085 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2600: 0.5229387488807745\n",
      "2024-06-13 16:34:30,457 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2700: 0.5229174926977644\n",
      "2024-06-13 16:34:30,814 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2800: 0.5228960339798321\n",
      "2024-06-13 16:34:31,181 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2900: 0.5228673638029481\n",
      "2024-06-13 16:34:31,679 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:262] - INFO: Current        h: 0.0021684930246799183\n",
      "2024-06-13 16:34:31,681 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:252] - INFO: Current epoch: 3==================\n",
      "2024-06-13 16:34:31,684 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 0: 0.5228170867624117\n",
      "2024-06-13 16:34:32,163 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 100: 0.5227433651077218\n",
      "2024-06-13 16:34:32,717 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 200: 0.5225378356519368\n",
      "2024-06-13 16:34:33,225 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 300: 0.5221351593476647\n",
      "2024-06-13 16:34:33,866 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 400: 0.5215387789611374\n",
      "2024-06-13 16:34:34,399 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 500: 0.5212621858125659\n",
      "2024-06-13 16:34:34,851 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 600: 0.5209832643623343\n",
      "2024-06-13 16:34:35,218 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 700: 0.5206559088784968\n",
      "2024-06-13 16:34:35,633 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 800: 0.5202767500372903\n",
      "2024-06-13 16:34:36,167 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 900: 0.519850803800811\n",
      "2024-06-13 16:34:36,658 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1000: 0.5193846500221821\n",
      "2024-06-13 16:34:37,188 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1100: 0.518924934471306\n",
      "2024-06-13 16:34:37,601 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1200: 0.5184161815499414\n",
      "2024-06-13 16:34:37,997 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1300: 0.5180152336630162\n",
      "2024-06-13 16:34:38,353 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1400: 0.5176344099923069\n",
      "2024-06-13 16:34:38,743 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1500: 0.5173738753433229\n",
      "2024-06-13 16:34:39,123 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1600: 0.517191235091432\n",
      "2024-06-13 16:34:39,481 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1700: 0.5170411087705415\n",
      "2024-06-13 16:34:39,862 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1800: 0.5169301407403522\n",
      "2024-06-13 16:34:40,235 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 1900: 0.5168369420084065\n",
      "2024-06-13 16:34:40,632 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2000: 0.5167231929216756\n",
      "2024-06-13 16:34:41,014 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2100: 0.5166523000766068\n",
      "2024-06-13 16:34:41,374 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2200: 0.516568713536055\n",
      "2024-06-13 16:34:41,746 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2300: 0.5164971284648412\n",
      "2024-06-13 16:34:42,089 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2400: 0.5164234123226833\n",
      "2024-06-13 16:34:42,436 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2500: 0.5163423733955969\n",
      "2024-06-13 16:34:42,790 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2600: 0.5162718701634738\n",
      "2024-06-13 16:34:43,197 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2700: 0.5161779161644708\n",
      "2024-06-13 16:34:43,578 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2800: 0.5160568405228515\n",
      "2024-06-13 16:34:43,975 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:298] - INFO: Current loss in step 2900: 0.5159173173345936\n",
      "2024-06-13 16:34:44,390 - C:\\Users\\q619374\\AppData\\Local\\Temp\\ipykernel_9588\\4139471359.py[line:262] - INFO: Current        h: 0.00048397986973380824\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fdr': 0.4, 'tpr': 0.75, 'fpr': 0.3333, 'shd': 3, 'nnz': 5, 'precision': 0.4286, 'recall': 0.75, 'F1': 0.5455, 'gscore': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\q619374\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\castle\\metrics\\evaluation.py:193: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  num_tp =  (W_p + W_true).applymap(lambda elem:1 if elem==2 else 0).sum(axis=1).sum()\n",
      "c:\\Users\\q619374\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\castle\\metrics\\evaluation.py:195: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  num_fn_r = (W_p - W_true).applymap(lambda elem:1 if elem==1 else 0).sum(axis=1).sum()\n",
      "c:\\Users\\q619374\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\castle\\metrics\\evaluation.py:221: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  TP = (W_p + W_true).applymap(lambda elem:1 if elem==2 else 0).sum(axis=1).sum()\n"
     ]
    }
   ],
   "source": [
    "dag = np.array([[0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]])\n",
    "dag = DAG.scale_free(n_nodes=43, n_edges=40)\n",
    "data = IIDSimulation(W=dag, n=1000, method='linear', sem_type='logistic')\n",
    "\n",
    "model = GAE(epochs=3)\n",
    "model.learn(data.X)\n",
    "\n",
    "GraphDAG(model.causal_matrix, data.B)\n",
    "metrics = MetricsDAG(model.causal_matrix, data.B)\n",
    "print(metrics.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
